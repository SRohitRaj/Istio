#
# addon kiali
#
kiali:
  enabled: false # Note that if using the demo or demo-auth yaml when installing via Helm, this default will be `true`.
  replicaCount: 1
  hub: quay.io/kiali
  tag: v1.18
  image: kiali
  contextPath: /kiali # The root context path to access the Kiali UI.
  nodeSelector: {}
  tolerations: []
  podAnnotations: {}

  # Specify the pod anti-affinity that allows you to constrain which nodes
  # your pod is eligible to be scheduled based on labels on pods that are
  # already running on the node rather than based on labels on nodes.
  # There are currently two types of anti-affinity:
  #    "requiredDuringSchedulingIgnoredDuringExecution"
  #    "preferredDuringSchedulingIgnoredDuringExecution"
  # which denote "hard" vs. "soft" requirements, you can define your values
  # in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
  # correspondingly.
  # For example:
  # podAntiAffinityLabelSelector:
  # - key: security
  #   operator: In
  #   values: S1,S2
  #   topologyKey: "kubernetes.io/hostname"
  # This pod anti-affinity rule says that the pod requires not to be scheduled
  # onto a node if that node is already running a pod with label having key
  # "security" and value "S1".
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []

  service:
    annotations: {}
    type: ClusterIP

  dashboard:
    auth:
      strategy: login # Can be anonymous, login, openshift, or ldap
      # ldap: # This is required to use the ldap strategy
      #   ldap_base: "DC=example,DC=com"
      #   ldap_bind_dn: "CN={USERID},OU=xyz,OU=Users,OU=Accounts,DC=example,DC=com"
      #   ldap_group_filter: "(cn=%s)"
      #   ldap_host: "ldap-service.ldap-namespace"
      #   ldap_insecure_skip_verify: true
      #   ldap_mail_id_key: "mail"
      #   ldap_member_of_key: "memberOf"
      #   ldap_port: 123
      #   ldap_role_filter: ".*xyz.*"
      #   ldap_search_filter: "(&(name={USERID}))"
      #   ldap_use_ssl: false
      #   ldap_user_filter: "(cn=%s)"
      #   ldap_user_id_key: "cn"

    secretName: kiali # You must create a secret with this name - one is not provided out-of-box unless you've chose to create a demo secret.
    usernameKey: username # This is the key name within the secret whose value is the actual username.
    passphraseKey: passphrase # This is the key name within the secret whose value is the actual passphrase.

    viewOnlyMode: false # Bind the service account to a role with only read access

    grafanaURL: "" # If you have Grafana installed and it is accessible to client browsers, then set this to its external URL. Kiali will redirect users to this URL when Grafana metrics are to be shown.
    grafanaInClusterURL: "http://grafana:3000" # In Kubernetes cluster with ELB in front this option is needed, since public IP of ELB is not reachable from inside the cluster
    jaegerURL: "" # If you have Jaeger installed and it is accessible to client browsers, then set this property to its external URL. Kiali will redirect users to this URL when Jaeger tracing is to be shown.
    jaegerInClusterURL: "http://tracing/jaeger" # If you have Jaeger installed and accessible from Kiali pod (typically in cluster), then set this property to enable more tracing charts within Kiali.

  createDemoSecret: true # When true, a secret will be created with a default username and password. Useful for demos.

  prometheusAddr: ""
  resources: {}
  security:
    enabled: false
    cert_file: /kiali-cert/cert-chain.pem
    private_key_file: /kiali-cert/key.pem

revision: ""

global:
  # Specify pod scheduling arch(amd64, ppc64le, s390x) and weight as follows:
  #   0 - Never scheduled
  #   1 - Least preferred
  #   2 - No preference
  #   3 - Most preferred
  arch:
    amd64: 2
    s390x: 2
    ppc64le: 2

  # Default node selector to be applied to all deployments so that all pods can be
  # constrained to run a particular nodes. Each component can overwrite these default
  # values by adding its node selector block in the relevant section below and setting
  # the desired values.
  defaultNodeSelector: {}

  # A minimal set of requested resources to applied to all deployments so that
  # Horizontal Pod Autoscaler will be able to function (if set).
  # Each component can overwrite these default values by adding its own resources
  # block in the relevant section below and setting the desired resources values.
  defaultResources:
    requests:
      cpu: 10m
    #   memory: 128Mi
    # limits:
    #   cpu: 100m
    #   memory: 128Mi

  # Default node tolerations to be applied to all deployments so that all pods can be
  # scheduled to a particular nodes with matching taints. Each component can overwrite
  # these default values by adding its tolerations block in the relevant section below
  # and setting the desired values.
  # Configure this field in case that all pods of Istio control plane are expected to
  # be scheduled to particular nodes with specified taints.
  defaultTolerations: []

  # Specify image pull policy if default behavior isn't desired.
  # Default behavior: latest images will be Always else IfNotPresent.
  imagePullPolicy: ""

  # ImagePullSecrets for all ServiceAccount, list of secrets in the same namespace
  # to use for pulling any images in pods that reference this ServiceAccount.
  # For components that don't use ServiceAccounts (i.e. grafana, servicegraph, tracing)
  # ImagePullSecrets will be added to the corresponding Deployment(StatefulSet) objects.
  # Must be set for any cluster configured with private docker registry.
  imagePullSecrets: []
  # - private-registry-key

  # Used to locate istio-pilot.
  # Default is to install pilot in a dedicated namespace, istio-pilot11. You can use multiple namespaces, but
  # for each 'profile' you need to match the control plane namespace and the value of istioNamespace
  # It is assumed that istio-system is running either 1.0 or an upgraded version of 1.1, but only security components are
  # used (citadel generating the secrets).
  istioNamespace: istio-system

  # Kubernetes >=v1.11.0 will create two PriorityClass, including system-cluster-critical and
  # system-node-critical, it is better to configure this in order to make sure your Istio pods
  # will not be killed because of low priority class.
  # Refer to https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  # for more detail.
  priorityClassName: ""

  prometheusNamespace: istio-system

  telemetryNamespace: istio-system
